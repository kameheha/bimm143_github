---
title: "class08"
author: "Victor Yu"
format: pdf
---
Today Mini-project we will explore a complete analysis using the unsupervised learning techniques covered in class (clustering and PCA for now)

```{r}
# Save your input data file into your Project directory
fna.data <-  "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <-  read.csv(fna.data, row.names=1)

head(wisc.df)
```

```{r}
#Data Frame We are Going to Work With
#Removing M / B column (which tells us the cancer we are working with)
wisc.data <- wisc.df[,-1]

diagnosis <- as.factor(wisc.df[,1])

head(wisc.data)
```

Q1. How many observations are in this dataset?
Answer: 569 observations

```{r}
dim(wisc.data)
```


Q2. How many of the observations have a malignant diagnosis?
Answer: 212 malignant diagnosis
```{r}
table (diagnosis, exclude = "B")

```


Q3. How many varaibles/features in the data are suffixed with _mean?
Answer: 10 variable / features are suffixed with _mean
```{r}
word_mean<- grep("_mean", colnames(wisc.data)) 
length(word_mean)
```

```{r}
#Check column means and start deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp (wisc.data, scale = TRUE)
summary(wisc.pr)
```

Q4: From your results, what proportion of the original variance is captured by the first principal components (PC1)
Answer: 44.27%

```{r}
s <- summary(wisc.pr)
s$importance ["Proportion of Variance","PC1"]
```


Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
   Answer: 3PCs capture 72%
```{r}
s$importance [3, ]
```

Q6. How many principal componenets (PCs) are required to describe at least 90% of the original variance in the data
Q6. Answer: 7 PCs
```{r}
s$importance[3,]
```


We need to make our plot of PC1 vs PC2 (a.k.a score plot, PC-plot, etc.)
The main resuult of PCA

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
Q7. Answers: The plot is difficult to understand. Its just a blob of data points
```{r}
#? Concentration scatter plot>
biplot(wisc.pr)
```

Q8. Generate a similar plot for principal componenets 1 and 3. What do you notice about these plots?
Q8. Answer: The plots in this graph is more spread out / easier to read. The distinction of the two points is a lot easier to see.

```{r}
#scatter plot
plot(wisc.pr$x[,1], wisc.pr$x[,3], xlab = "PC1", ylab= "PC3", col = diagnosis)
```


```{r}
#Create a data.frame for ggplot
df <- as.data.frame (wisc.pr$x)
df$diagnosis <- diagnosis

#Load the ggplot2 package
library (ggplot2)
#Make a scatter plot colored 
ggplot (df) + aes (PC1, PC2, col = diagnosis) +
  geom_point()
```

```{r}
#Calculated variance of each component
pr.var <-  wisc.pr$sdev^2
head(pr.var)
```

```{r}
#Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

#Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0, .5), type = "o")
```

```{r}
#Alternative scree plot of the same data, note data
barplot(pve, ylab = "Percent of Variance Explained",
        names.arg=paste0("PC", 1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

OPTION Graph!
```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

Q9: For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

Q9. Answer: -0.26085376 

```{r}
#,1 focused on PC1
wisc.pr$rotation[,1]
```

Q10: What is the minimum number of principal components required to explain 80% of the variance of the data?
Q10. Answer: 5 PC
```{r}
s$importance[3,]
```
There is a commplicated mix of variables that go toegheter to make up PC1 - i.e. there are many of the original viarables that together contribute highly to PC1.

#3 Heirarchical Clustering

The goal of this section is to do hierarchical clustering original data. Firt we will scale the data, then distance matrix.

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
Q11. ANSWER: Around 19
```{r}
#setting variables
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method =)

plot (wisc.hclust)
abline(h=19, col = "red", lty = 2)
```


Cut this tree to yield cluster memberhsip vector with `cutree()` function

Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
Answer: No, I can't maybe there is a better method, but at my current ability i cant. 4 looks pretty good out of all of them. It divides the cluster nicely to our desired result / goal we are going for.

```{r}
wisc.hclust.clusters <- cutree (wisc.hclust, k =4 )

table (wisc.hclust.clusters, diagnosis)

```

```{r}
#Forms a cross table, so you can read data points cleanly
#Code is not rendering since the variable wisc.pr.hclust was used later. As a result, im going to double call it just to fix the issue.

plot(wisc.pr$x[,1], wisc.pr$x[,2], xlab = "PC1", ylab = "PC2", col = diagnosis)

#Cluster the PCA results
d <-  dist (wisc.pr$x[,1:3])

wisc.pr.hclust <- hclust(d, method = "ward.D2")

grps <- cutree(wisc.pr.hclust, k=2)

table(grps)

table(grps, diagnosis)
```

Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
Answer: To be honest, I can't really tell the between the 4 methods. Maybe it is our dataset, but it look the same. As a result, I would probably go with complete since I'll remember complete rather than the other 4 methods.

#Combine Methods PCA

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], xlab = "PC1", ylab = "PC2", col = diagnosis)

#Cluster the PCA results
d <-  dist (wisc.pr$x[,1:3])

wisc.pr.hclust <- hclust(d, method = "ward.D2")
```


An my tree result figure
```{r}
#Cleaner Tree
plot (wisc.pr.hclust)
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)
```


```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table (grps, diagnosis)
```

```{r}
plot(wisc.pr$x [,1:2], col = grps)
plot(wisc.pr$x[,1:2], col=diagnosis)
```

Q15. How well does the newly created model with four clusters separate out the two diagnoses?
Answer: it separates the model really nicely. Cleaner to see the observation numbers. Easier way to see TN, TP, FN , FP, so this cluster is good.
```{r}
#For the Hclust, you need to put in the distance function. REMMEMBER THAT
wisc.pr.hclust <- hclust( dist(wisc.pr$x[,1:7]), method="ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)

table(wisc.pr.hclust.clusters, diagnosis)
```



